{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "r09631017_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3PKRLTwphQnl",
        "2Yxxi8S4hQnm",
        "9o60wNwhhQnm",
        "7KJvZ4bShQnn",
        "mcy2PXbfhQnn",
        "ngkc_b4jhQno",
        "jemoHS_FhQnp",
        "nWqdIuTrhQnq",
        "HSEgbSHphQnq",
        "CTvAuanWhQnr"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86d9489993bf45f2ac4b923a58d9ba7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_92827c79c1bc446296be1da2aa04bb8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0709ee5e37da49d0b35098107c2f38c3",
              "IPY_MODEL_b18ddb65533b4543a65954c26a8915db"
            ]
          }
        },
        "92827c79c1bc446296be1da2aa04bb8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0709ee5e37da49d0b35098107c2f38c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_977e155d2ad742ffa5f5b84b4cc7887d",
            "_dom_classes": [],
            "description": "train epoch 29: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1914,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1914,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69ced8889c124ce88a5972890f7e94a7"
          }
        },
        "b18ddb65533b4543a65954c26a8915db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3112a7ea071648609694d8f25ea8e91e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1914/1914 [46:38&lt;00:00,  1.25s/it, loss=2.25]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0c1d9ee11dc4ae3901e7b537295b167"
          }
        },
        "977e155d2ad742ffa5f5b84b4cc7887d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69ced8889c124ce88a5972890f7e94a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3112a7ea071648609694d8f25ea8e91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0c1d9ee11dc4ae3901e7b537295b167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "870be36cc34749098c41b102cfcd0132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1207a00b3287448ea0e5249a0b04df36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_baed02fe2b2b4e4f8f925b06f88ac6c6",
              "IPY_MODEL_0969900ef2cd408ea5e24419624954e7"
            ]
          }
        },
        "1207a00b3287448ea0e5249a0b04df36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "baed02fe2b2b4e4f8f925b06f88ac6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ce38c7c096d43ff94564d53504a7591",
            "_dom_classes": [],
            "description": "validation: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 23,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f84f6253f14443f7be79f31cb524bba5"
          }
        },
        "0969900ef2cd408ea5e24419624954e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45f04dad98de4927801e10fee8ac27e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23/23 [01:29&lt;00:00,  4.74s/it, valid_loss=3.14]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0192ff459d642cdb50e5eb5072999eb"
          }
        },
        "0ce38c7c096d43ff94564d53504a7591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f84f6253f14443f7be79f31cb524bba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45f04dad98de4927801e10fee8ac27e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0192ff459d642cdb50e5eb5072999eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bddfc342dc4b4bab88bb9ba5a636b2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13ade1ba79f9406f9e2c3924c45d1bf7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c07c36d407840fcb186b85542cacc2c",
              "IPY_MODEL_609db06dee4c4d64acc19618ef5bb5f9"
            ]
          }
        },
        "13ade1ba79f9406f9e2c3924c45d1bf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c07c36d407840fcb186b85542cacc2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_79d98454e097448f86c5d584a3db2823",
            "_dom_classes": [],
            "description": "train epoch 30: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1914,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1914,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab5c8fdb470046278f0f7fd37c9dc9c3"
          }
        },
        "609db06dee4c4d64acc19618ef5bb5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_361ae03075024454a8a8e16362f1cdd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1914/1914 [46:48&lt;00:00,  1.23s/it, loss=2.21]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4650fb2c94a446e5bc4ccc34e56f1584"
          }
        },
        "79d98454e097448f86c5d584a3db2823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab5c8fdb470046278f0f7fd37c9dc9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "361ae03075024454a8a8e16362f1cdd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4650fb2c94a446e5bc4ccc34e56f1584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9951460a0997473d8bf95fcb71ef40f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_910332b07a394fa1a7b5b97af1770a8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33710ee24c8242468b5124ccefb92f86",
              "IPY_MODEL_f6fd4bd2e6dc49f5a5088d7bbe794346"
            ]
          }
        },
        "910332b07a394fa1a7b5b97af1770a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33710ee24c8242468b5124ccefb92f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04d0a69ef70142b3bcdba8f2e01d5b25",
            "_dom_classes": [],
            "description": "validation: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 23,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7032276ab2df400b9f484bb3a5d9a870"
          }
        },
        "f6fd4bd2e6dc49f5a5088d7bbe794346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bbe1ff6b9c2e43ac872e9fcefb145813",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23/23 [01:26&lt;00:00,  4.37s/it, valid_loss=3.14]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8701a54ac584e1ebc21e82ed8d15151"
          }
        },
        "04d0a69ef70142b3bcdba8f2e01d5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7032276ab2df400b9f484bb3a5d9a870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbe1ff6b9c2e43ac872e9fcefb145813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8701a54ac584e1ebc21e82ed8d15151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "964bc9e3acf5403e9742e08263eed7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a459d6ac8f84e139934455b6316f5f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0367d713ffa741dabd7dc47e590ca079",
              "IPY_MODEL_b6c00da1bc1f4818adc156de68d2b255"
            ]
          }
        },
        "6a459d6ac8f84e139934455b6316f5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0367d713ffa741dabd7dc47e590ca079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ef90aeb1236430da62f752e37e42d2b",
            "_dom_classes": [],
            "description": "validation: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 23,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_345b58b5aa484c0994eb069625c72cb2"
          }
        },
        "b6c00da1bc1f4818adc156de68d2b255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0c22563f83f94b17980044da686616e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23/23 [01:25&lt;00:00,  4.53s/it, valid_loss=3.11]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_596147d186ca4ab2987329ab6835a9b6"
          }
        },
        "7ef90aeb1236430da62f752e37e42d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "345b58b5aa484c0994eb069625c72cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c22563f83f94b17980044da686616e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "596147d186ca4ab2987329ab6835a9b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2342017fc61d48b685c59307878df153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_69d4b0bb74d2427fa28b3c76100d897b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ed9c921ba22434bb3c3759246d6ce19",
              "IPY_MODEL_f51f4f3d399646e08d319ff7ea867367"
            ]
          }
        },
        "69d4b0bb74d2427fa28b3c76100d897b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ed9c921ba22434bb3c3759246d6ce19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a68f9b74a1d24e41ae3993188d82a9a1",
            "_dom_classes": [],
            "description": "prediction: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 17,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 17,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57552d5cefcf4fd680f02935d483efc1"
          }
        },
        "f51f4f3d399646e08d319ff7ea867367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2244681acb8343aaade4fc7d827b5598",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 17/17 [01:09&lt;00:00,  4.07s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dd9dfad62b14d9c9e1d1034dd106b6a"
          }
        },
        "a68f9b74a1d24e41ae3993188d82a9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57552d5cefcf4fd680f02935d483efc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2244681acb8343aaade4fc7d827b5598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dd9dfad62b14d9c9e1d1034dd106b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCAvmVnOMayl"
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvvMxZWz1fBH",
        "outputId": "d51f3297-ba44-45b0-c13c-29c5c79766cc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 26 06:29:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VYVT5SRhQne"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9NnQbCmiIkx",
        "outputId": "819f49c1-b918-43b9-ab44-a36c2c79b562"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp5yjHZUhQne",
        "outputId": "12d5bce8-c990-49fd-98ac-5f8270e7e747"
      },
      "source": [
        "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "!pip install --upgrade jupyter ipywidgets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 10.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/28/4aefc543967839bdb4e139831b82004279f1c435cede2a9557ccf8369875/wandb-0.10.27-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.0MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.0MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=dfce54888f93a9876642f1a714ed691b7547d9c5655688728915ecf28257db35\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=ee245e3d11fe19819d7728455a0a4bf5d751459ab527bc3602c44620f8391957\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: portalocker, sacrebleu, sacremoses, sentencepiece, configparser, docker-pycreds, pathtools, sentry-sdk, subprocess32, shortuuid, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.27\n",
            "Requirement already up-to-date: jupyter in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already up-to-date: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.0.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (4.7.1)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook->jupyter) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG6Q-luZhQne",
        "outputId": "bcb514cb-10dd-4031-c56a-63973a9169fa"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 27710, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 27710 (delta 60), reused 99 (delta 54), pack-reused 27579\u001b[K\n",
            "Receiving objects: 100% (27710/27710), 11.54 MiB | 29.11 MiB/s, done.\n",
            "Resolving deltas: 100% (20879/20879), done.\n",
            "Note: checking out '9a1c497'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 9a1c4970 Make Hydra logging work with DDP (#1568)\n",
            "Processing ./fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (0.29.22)\n",
            "Requirement already satisfied, skipping upgrade: numpy; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.19.5)\n",
            "Collecting hydra-core<1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.5.1)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+9a1c497) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+9a1c497) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (5.1.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (3.4.1)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+9a1c497-cp37-cp37m-linux_x86_64.whl size=2817627 sha256=11d11c091a61183ce3391ba5b3355732ff26ed3bb9a3a036cfd4dc866096bdc9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_hqh0nw4/wheels/94/b2/67/6399f5bcb823dc3a8b1e84965aaae15af9ed863fee98a59129\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=7967c744e03e044a4d7511a56756a4af7cb676b02df46dd85296a59a1e3a3e5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-1.0.0a0+9a1c497 hydra-core-1.0.6 omegaconf-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awG518aqhQnf"
      },
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqeZoicFhQnf"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUtW29WchQng"
      },
      "source": [
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhjxkHP-hQng"
      },
      "source": [
        "# Dataset Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhyByuDwhQnh"
      },
      "source": [
        "## Download and extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZLZxZHuhQnh",
        "outputId": "013e6786-35c8-4eaa-b2e4-e2e6b5fc127e"
      },
      "source": [
        "data_dir = 'gdrive/MyDrive/DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\"', \n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz\",\n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz\",\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\",\n",
        "#     \"https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = prefix/f\n",
        "    if not path.exists():\n",
        "        if 'mega' in u:\n",
        "            !megadl {u} --path {path}\n",
        "        else:\n",
        "            !wget {u} -O {path}\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw.en\n",
            "raw.zh\n",
            "test.en\n",
            "test.zh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDwINxdwhQnh"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqG4tNSVhQnh"
      },
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXtUp394hQnh",
        "outputId": "87af2896-0c41-4303-9833-27fa3b998219"
      },
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa5DZbGthQnh"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwpyxeo4hQni"
      },
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVRCjX5ahQni",
        "outputId": "1ea49166-0ce5-4bb3-bb94-d209f118c47f"
      },
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNnikruRhQni",
        "outputId": "9a35ad87-63ee-4a13-85a7-7c5d8edb4f24"
      },
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dqv9mvnhQnj"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpPrfmc2hQnj"
      },
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiXcN_xwhQnj",
        "outputId": "6fe08309-d95b-48e0-96b5-a1d900e0085a"
      },
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pYfOOh5hQnj"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_1l1lK5hQnj",
        "outputId": "ce5c9307-5cfb-45d5-fe7f-9b73731a57ab"
      },
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eU9PGbXhQnk",
        "outputId": "86053237-5f5f-4340-baa5-a506a0e7d273"
      },
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
            "/content/gdrive/MyDrive/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVxBXJgyhQnk",
        "outputId": "716cefc5-c17a-45f6-c15a-486fc5fbe4e6"
      },
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁ ; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl own ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁ 。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁ 。 ▁我 非常 感 激 ▁ 。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對 我 之前 演講 的 好 評 ▁ 。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁ !\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁ !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJe6g4TJhQnk"
      },
      "source": [
        "## Binarize the data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBvV8X8dhQnk",
        "outputId": "8a1dddd5-6b7a-4b37-9ef2-2e1dcfb450fb"
      },
      "source": [
        "binpath = Path('gdrive/MyDrive/DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/MyDrive/DATA/data-bin/ted2020 exists, will not overwrite!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaMh2l0IhQnk"
      },
      "source": [
        "# Configuration for Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83dRSaLYhQnk"
      },
      "source": [
        "\n",
        "\n",
        "#back-translation config\n",
        "# datadir = \"./gdrive/MyDrive/DATA/data-bin/ted2020\",\n",
        "# savedir = \"./gdrive/MyDrive/checkpoints/transformer-back\",\n",
        "# source_lang = \"zh\",\n",
        "# target_lang = \"en\",\n",
        "\n",
        "#ted2020 with mono config\n",
        "# datadir = \"./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono\",\n",
        "# savedir = \"./gdrive/MyDrive/checkpoints/ted2020-mono\",\n",
        "# source_lang = \"en\",\n",
        "# target_lang = \"zh\",\n",
        "\n",
        "config = Namespace(\n",
        "    datadir = \"./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"./gdrive/MyDrive/checkpoints/ted2020-mono\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=1.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=29,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibdZ4sgthQnk"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFJQ641GhQnl",
        "scrolled": true
      },
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axoe3wKmhQnl"
      },
      "source": [
        "# CUDA Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxMgUmM9hQnl",
        "outputId": "0a4460f6-cf80-4d2d-af21-065dbf318bda"
      },
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:31:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-04-26 06:31:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2021-04-26 06:31:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PKRLTwphQnl"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crNprglfhQnl"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwPHwAwLhQnl",
        "outputId": "41a40f31-c0a9-4632-c499-ae6a1216d064"
      },
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:31:46 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2021-04-26 06:31:46 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYFPbZuihQnl",
        "outputId": "b1d0bfc6-20d4-44d0-b6c2-f6932c07a43b"
      },
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:31:46 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2021-04-26 06:31:47 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train.en-zh.en\n",
            "2021-04-26 06:31:48 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train.en-zh.zh\n",
            "2021-04-26 06:31:48 | INFO | fairseq.tasks.translation | ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2021-04-26 06:31:50 | INFO | fairseq.data.data_utils | loaded 782,527 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
            "2021-04-26 06:31:52 | INFO | fairseq.data.data_utils | loaded 782,527 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
            "2021-04-26 06:31:52 | INFO | fairseq.tasks.translation | ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono train1 en-zh 782527 examples\n",
            "2021-04-26 06:31:52 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/valid.en-zh.en\n",
            "2021-04-26 06:31:53 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/valid.en-zh.zh\n",
            "2021-04-26 06:31:53 | INFO | fairseq.tasks.translation | ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3FbX6YShQnm",
        "outputId": "38c0bf52-ab50-45d3-b746-875fd4647cef"
      },
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2218,   60,   19,   75,    4,  253,   16,  334, 1392,\n",
            "        1689,    7,    2]),\n",
            " 'target': tensor([ 145,  684,   30,  270,   40,  168, 1134,  650,  591,  367, 3117, 2417,\n",
            "        1420,  194,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsIPSmvhQnm"
      },
      "source": [
        "## Dataset Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QekHP2yPhQnm",
        "outputId": "d2a76969-7996-427e-c4c8-9060e87427ed"
      },
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:31:53 | WARNING | fairseq.tasks.fairseq_task | 2,586 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 2444, 135, 3058, 93, 2275, 682, 2649, 731, 1623]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': tensor([3200,  828]),\n",
              " 'net_input': {'prev_output_tokens': tensor([[   2,    4,  237, 1769,    9,  494,  491,  670,    4,   10,    1,    1,\n",
              "              1,    1,    1,    1],\n",
              "          [   2,    4,  325, 1974,  793,  294,  289,  596,    4,   10,    1,    1,\n",
              "              1,    1,    1,    1]]),\n",
              "  'src_lengths': tensor([8, 8]),\n",
              "  'src_tokens': tensor([[ 159, 2479,   17,  131,   72,  733,    7,    2],\n",
              "          [  81,    4, 1709,    4,  862,  718,    7,    2]])},\n",
              " 'nsentences': 2,\n",
              " 'ntokens': 20,\n",
              " 'target': tensor([[   4,  237, 1769,    9,  494,  491,  670,    4,   10,    2,    1,    1,\n",
              "             1,    1,    1,    1],\n",
              "         [   4,  325, 1974,  793,  294,  289,  596,    4,   10,    2,    1,    1,\n",
              "             1,    1,    1,    1]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yxxi8S4hQnm"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hFx6KI3hQnm"
      },
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o60wNwhhQnm"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjuyOh7bhQnn"
      },
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "        self.num_layers = args.encoder_layers\n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        self.padding_idx = dictionary.pad()\n",
        "        \n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused):\n",
        "        bsz, seqlen = src_tokens.size()\n",
        "        \n",
        "        # get embeddings\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # pass thru bidirectional RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        \n",
        "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "        \n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "                encoder_padding_mask,  # seq_len x batch\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KJvZ4bShQnn"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2oFeZnOhQnn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcy2PXbfhQnn"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzgyb_SVhQno"
      },
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "        \n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "        \n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attention = AttentionLayer(\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        ) \n",
        "        # self.attention = None\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "        \n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_(\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "        \n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        # extract the outputs from encoder\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "        \n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
        "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
        "            prev_hiddens = encoder_hiddens\n",
        "        \n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "        \n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "                \n",
        "        # decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "                        \n",
        "        # pass thru unidirectional RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "                \n",
        "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
        "        # we need to do an extra projection)\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "        \n",
        "        # project to vocab size\n",
        "        x = self.output_projection(x)\n",
        "        \n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "        \n",
        "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        \n",
        "        return x, None\n",
        "    \n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngkc_b4jhQno"
      },
      "source": [
        "## Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOdZXOD_hQnp"
      },
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jemoHS_FhQnp"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G3nT4SYhQnp"
      },
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    \n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfnmDt8VhQnp"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "reference implementation\n",
        "\n",
        "|model|embedding dim|encoder ffn|encoder layers|decoder ffn|decoder layers|\n",
        "|-|-|-|-|-|-|\n",
        "|RNN|256|512|1|1024|1|\n",
        "|Transformer|256|1024|4|1024|4|\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fqbu96xhQnp"
      },
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=1024,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=1024,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3,\n",
        ")\n",
        "\n",
        "# # HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhhwjG4VhQnp"
      },
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSVPI1-BhQnp",
        "outputId": "e5e27267-559a-4912-a94c-35e5c8fdcf7f"
      },
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:31:57 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWqdIuTrhQnq"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIFBnRrshQnq"
      },
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNnp74tVhQnq"
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfp8FXSthQnq"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yee324BDhQnq",
        "outputId": "706ad267-0fa3-495e-a445-8824d2c6267a"
      },
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEgbSHphQnq"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJsgoFkihQnr"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQIf6jA8hQnr"
      },
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzGtYfxkhQnr"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9nHI58rhQnr"
      },
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvreN1-nhQnr"
      },
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvAuanWhQnr"
      },
      "source": [
        "# Save and Load Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvjXcHzJhQnr"
      },
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpw2jtPfhQns"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgLaAzN6hQns"
      },
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw1-I1O1hQns",
        "outputId": "1e0c43e6-d0b9-47a1-9ba4-3512f4f49010"
      },
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | num. model params: 142,389,248 (num. trained: 142,389,248)\n",
            "2021-04-26 06:32:08 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "86d9489993bf45f2ac4b923a58d9ba7c",
            "92827c79c1bc446296be1da2aa04bb8c",
            "0709ee5e37da49d0b35098107c2f38c3",
            "b18ddb65533b4543a65954c26a8915db",
            "977e155d2ad742ffa5f5b84b4cc7887d",
            "69ced8889c124ce88a5972890f7e94a7",
            "3112a7ea071648609694d8f25ea8e91e",
            "f0c1d9ee11dc4ae3901e7b537295b167",
            "870be36cc34749098c41b102cfcd0132",
            "1207a00b3287448ea0e5249a0b04df36",
            "baed02fe2b2b4e4f8f925b06f88ac6c6",
            "0969900ef2cd408ea5e24419624954e7",
            "0ce38c7c096d43ff94564d53504a7591",
            "f84f6253f14443f7be79f31cb524bba5",
            "45f04dad98de4927801e10fee8ac27e8",
            "a0192ff459d642cdb50e5eb5072999eb",
            "bddfc342dc4b4bab88bb9ba5a636b2a7",
            "13ade1ba79f9406f9e2c3924c45d1bf7",
            "5c07c36d407840fcb186b85542cacc2c",
            "609db06dee4c4d64acc19618ef5bb5f9",
            "79d98454e097448f86c5d584a3db2823",
            "ab5c8fdb470046278f0f7fd37c9dc9c3",
            "361ae03075024454a8a8e16362f1cdd5",
            "4650fb2c94a446e5bc4ccc34e56f1584",
            "9951460a0997473d8bf95fcb71ef40f9",
            "910332b07a394fa1a7b5b97af1770a8b",
            "33710ee24c8242468b5124ccefb92f86",
            "f6fd4bd2e6dc49f5a5088d7bbe794346",
            "04d0a69ef70142b3bcdba8f2e01d5b25",
            "7032276ab2df400b9f484bb3a5d9a870",
            "bbe1ff6b9c2e43ac872e9fcefb145813",
            "c8701a54ac584e1ebc21e82ed8d15151"
          ]
        },
        "id": "6c-kSs2UhQns",
        "scrolled": true,
        "outputId": "cb445c89-03a7-4fc5-f27d-53bc60bd1dde"
      },
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 06:32:08 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2021-04-26 06:32:14 | INFO | hw5.seq2seq | loaded checkpoint gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint_last.pt: step=53235 loss=2.878422737121582 bleu=30.560291754710615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d9489993bf45f2ac4b923a58d9ba7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='train epoch 29', max=1914.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r2021-04-26 07:18:53 | INFO | hw5.seq2seq | training loss: 2.2747\n",
            "2021-04-26 07:18:53 | INFO | hw5.seq2seq | begin validation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "870be36cc34749098c41b102cfcd0132",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='validation', max=23.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r2021-04-26 07:20:24 | INFO | hw5.seq2seq | example source: right , it's gone ? or is it still up there ?\n",
            "2021-04-26 07:20:24 | INFO | hw5.seq2seq | example hypothesis: 沒錯 , 它不見了 ? 還是它還在上面 ?\n",
            "2021-04-26 07:20:24 | INFO | hw5.seq2seq | example reference: 還是還在 ?\n",
            "2021-04-26 07:20:24 | INFO | hw5.seq2seq | validation loss:\t2.8726\n",
            "2021-04-26 07:20:24 | INFO | hw5.seq2seq | BLEU = 30.68 60.7/37.1/24.0/16.4 (BP = 1.000 ratio = 1.008 hyp_len = 112758 ref_len = 111811)\n",
            "2021-04-26 07:20:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /content/gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint29.pt\n",
            "2021-04-26 07:20:49 | INFO | hw5.seq2seq | end of epoch 29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bddfc342dc4b4bab88bb9ba5a636b2a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='train epoch 30', max=1914.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r2021-04-26 08:07:38 | INFO | hw5.seq2seq | training loss: 2.2638\n",
            "2021-04-26 08:07:38 | INFO | hw5.seq2seq | begin validation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9951460a0997473d8bf95fcb71ef40f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='validation', max=23.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r2021-04-26 08:09:06 | INFO | hw5.seq2seq | example source: it's a ways out still .\n",
            "2021-04-26 08:09:06 | INFO | hw5.seq2seq | example hypothesis: 它仍然是個出路 。\n",
            "2021-04-26 08:09:06 | INFO | hw5.seq2seq | example reference: 這計畫目前還在進行 。\n",
            "2021-04-26 08:09:06 | INFO | hw5.seq2seq | validation loss:\t2.8727\n",
            "2021-04-26 08:09:06 | INFO | hw5.seq2seq | BLEU = 30.58 61.2/37.5/24.2/16.5 (BP = 0.987 ratio = 0.987 hyp_len = 110388 ref_len = 111811)\n",
            "2021-04-26 08:09:11 | INFO | hw5.seq2seq | saved epoch checkpoint: /content/gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint30.pt\n",
            "2021-04-26 08:09:11 | INFO | hw5.seq2seq | end of epoch 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Wf1-LfhQns"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd-F3EuuhQns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0d7033-ef1c-45c2-db8c-3a91a307cf50"
      },
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['./gdrive/MyDrive/checkpoints/ted2020-mono'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./gdrive/MyDrive/checkpoints/ted2020-mono/avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['./gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint30.pt', './gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint29.pt', './gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint28.pt', './gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint27.pt', './gdrive/MyDrive/checkpoints/ted2020-mono/checkpoint26.pt']\n",
            "Finished writing averaged checkpoint to ./gdrive/MyDrive/checkpoints/ted2020-mono/avg_last_5_checkpoint.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg0F25QEhQns"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Du-tIUghQns",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "964bc9e3acf5403e9742e08263eed7e8",
            "6a459d6ac8f84e139934455b6316f5f5",
            "0367d713ffa741dabd7dc47e590ca079",
            "b6c00da1bc1f4818adc156de68d2b255",
            "7ef90aeb1236430da62f752e37e42d2b",
            "345b58b5aa484c0994eb069625c72cb2",
            "0c22563f83f94b17980044da686616e0",
            "596147d186ca4ab2987329ab6835a9b6"
          ]
        },
        "outputId": "862c1173-bce0-44c5-aa26-5d754d2b7f54"
      },
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 08:10:25 | INFO | hw5.seq2seq | loaded checkpoint gdrive/MyDrive/checkpoints/ted2020-mono/avg_last_5_checkpoint.pt: step=unknown loss=2.8726627826690674 bleu=30.575877293673486\n",
            "2021-04-26 08:10:25 | INFO | hw5.seq2seq | begin validation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "964bc9e3acf5403e9742e08263eed7e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='validation', max=23.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r2021-04-26 08:11:53 | INFO | hw5.seq2seq | example source: fear not .\n",
            "2021-04-26 08:11:53 | INFO | hw5.seq2seq | example hypothesis: 恐懼不是 。\n",
            "2021-04-26 08:11:53 | INFO | hw5.seq2seq | example reference: 別怕 。\n",
            "2021-04-26 08:11:53 | INFO | hw5.seq2seq | validation loss:\t2.8601\n",
            "2021-04-26 08:11:53 | INFO | hw5.seq2seq | BLEU = 31.03 61.6/37.9/24.7/17.0 (BP = 0.987 ratio = 0.987 hyp_len = 110313 ref_len = 111811)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqnPCf3MhQnt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXCUGD_ZhQnt"
      },
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./gdrive/MyDrive/prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUftzXFYhQnt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "2342017fc61d48b685c59307878df153",
            "69d4b0bb74d2427fa28b3c76100d897b",
            "9ed9c921ba22434bb3c3759246d6ce19",
            "f51f4f3d399646e08d319ff7ea867367",
            "a68f9b74a1d24e41ae3993188d82a9a1",
            "57552d5cefcf4fd680f02935d483efc1",
            "2244681acb8343aaade4fc7d827b5598",
            "2dd9dfad62b14d9c9e1d1034dd106b6a"
          ]
        },
        "outputId": "da798e91-dc51-45ec-93ba-0cab4512fbe3"
      },
      "source": [
        "generate_prediction(model, task)\n",
        "#generate_prediction(model, task, 'mono', \"./gdrive/MyDrive/mono_prediction.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 08:11:53 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/test.en-zh.en\n",
            "2021-04-26 08:11:54 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/test.en-zh.zh\n",
            "2021-04-26 08:11:54 | INFO | fairseq.tasks.translation | ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2342017fc61d48b685c59307878df153",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='prediction', max=17.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Up3rPKhQnt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "ba6f4344-2a87-4dff-8578-ea9047655465"
      },
      "source": [
        "raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-9c9a2cba73bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiL-Q1pXhQnt"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXp46J6ohQnt"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzoqf-thQnt"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HXpz6nPhQnt"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51KGMl00hQnt"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjuLjTyghQnt"
      },
      "source": [
        "mono_dataset_name = 'mono'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5h-tjLKhQnu"
      },
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "urls = (\n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz\",\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = mono_prefix/f\n",
        "    if not path.exists():\n",
        "        if 'mega' in u:\n",
        "            !megadl {u} --path {path}\n",
        "        else:\n",
        "            !wget {u} -O {path}\n",
        "    else:\n",
        "        print(f'{f} is exist, skip downloading')\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "    elif path.suffix == \".gz\":\n",
        "        !gzip -fkd {path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJOkmP6hQnu"
      },
      "source": [
        "#TODO: clean corpus\n",
        "def mono_clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "            with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                for s1 in l1_in_f:\n",
        "                    s1 = s1.strip()\n",
        "                    s2 = '。'    #create a label for en.\n",
        "                    s1 = clean_s(s1, l1)\n",
        "                    s2 = clean_s(s2, l2)\n",
        "                    s1_len = len_s(s1, l1)\n",
        "                    s2_len = len_s(s2, l2)\n",
        "                    if min_len > 0: # remove short sentence\n",
        "                        if s1_len < min_len or s2_len < min_len:\n",
        "                            continue\n",
        "                    if max_len > 0: # remove long sentence\n",
        "                        if s1_len > max_len or s2_len > max_len:\n",
        "                            continue\n",
        "                    if ratio > 0: # remove by ratio of length\n",
        "                        if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                            continue\n",
        "                    print(s1, file=l1_out_f)\n",
        "                    print(s2, file=l2_out_f)\n",
        "                        \n",
        "mono_test_prefix = f'{mono_prefix}/ted_zh_corpus.deduped'                     \n",
        "mono_clean_corpus(mono_test_prefix, 'zh', 'en', ratio=-1, min_len=-1, max_len=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKtW5jkYhQnv"
      },
      "source": [
        "#TODO: Subword Units\n",
        "\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'mono': 'ted_zh_corpus.deduped.clean',\n",
        "}\n",
        "for split in ['mono']:\n",
        "    for lang in ['zh', 'en']:\n",
        "        out_path = mono_prefix/f'{split}.tok.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(mono_prefix/f'{split}.tok.{lang}', 'w') as out_f:\n",
        "                with open(mono_prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZgZ11OuaNkQ"
      },
      "source": [
        "!head {data_dir+'/'+mono_dataset_name+'/mono.tok.zh'} -n 5\n",
        "!head {data_dir+'/'+mono_dataset_name+'/mono.tok.en'} -n 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6nCHxbhQnv"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yEPGL31hQnv"
      },
      "source": [
        "binpath = Path('./gdrive/MyDrive/DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './gdrive/MyDrive/DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWoNPtJThQnv"
      },
      "source": [
        "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/mono/train.zh-en.zh.bin ./gdrive/MyDrive/DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/mono/train.zh-en.zh.idx ./gdrive/MyDrive/DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/mono/train.zh-en.en.bin ./gdrive/MyDrive/DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/mono/train.zh-en.en.idx ./gdrive/MyDrive/DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHZr5hbXKYym"
      },
      "source": [
        "raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx8cN2NrhQnv"
      },
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4w1Up1t0azl"
      },
      "source": [
        "generate_prediction(model, task, 'mono', \"./gdrive/MyDrive/mono_prediction.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB4obCBZah9E"
      },
      "source": [
        "## after prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOEwoyS3alIo"
      },
      "source": [
        "# rename original mono.tok.en  (▁ 。)\n",
        "!mv ./gdrive/MyDrive/DATA/rawdata/mono/mono.tok.en ./gdrive/MyDrive/DATA/rawdata/mono/old_mono.tok.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn3j2W0LhQnv"
      },
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "for split in ['mono']:\n",
        "    for lang in ['en']:\n",
        "        out_path = mono_prefix/f'{split}.tok.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(mono_prefix/f'{split}.tok.{lang}', 'w') as out_f:\n",
        "                with open('./gdrive/MyDrive/mono_prediction.txt', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)\n",
        "                        \n",
        "# hint: use fairseq to binarize these two files again\n",
        "binpath = Path('./gdrive/MyDrive/DATA/data-bin/synthetic')\n",
        "src_dict_file = './gdrive/MyDrive/DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvKRM9vhQnw"
      },
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./gdrive/MyDrive/DATA/data-bin/ted2020/ ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/synthetic/train.zh-en.zh.bin ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/synthetic/train.zh-en.zh.idx ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/synthetic/train.zh-en.en.bin ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./gdrive/MyDrive/DATA/data-bin/synthetic/train.zh-en.en.idx ./gdrive/MyDrive/DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}